# Performance Optimizations Guide

## Current Performance Issues & Solutions

### 1. ‚ö° Caching Layer

**Problem**: Repeated data loading from disk (Excel files, NetCDF files, JSON)
**Impact**: High I/O overhead, slow response times

**Solution**: Multi-level caching strategy

```python
# Install: pip install flask-caching redis
from flask_caching import Cache

# In app.py
cache = Cache(app.server, config={
    'CACHE_TYPE': 'redis',  # or 'filesystem' for simple cases
    'CACHE_REDIS_URL': 'redis://localhost:6379/0',
    'CACHE_DEFAULT_TIMEOUT': 300  # 5 minutes
})

# Usage in callbacks
@cache.memoize(timeout=600)  # 10 minutes
def load_network_data(project_path, network_file):
    # Expensive network loading
    return data
```

**Performance Gain**: 10-100x faster for repeated data access

---

### 2. üîÑ Background Task Processing

**Problem**: Long-running tasks (forecasting, PyPSA optimization) block the main thread
**Impact**: UI freezes, poor user experience, timeout issues

**Solution**: Celery + Redis for async task processing

```python
# Install: pip install celery redis
from celery import Celery

celery_app = Celery('kseb', broker='redis://localhost:6379/1')

@celery_app.task
def run_forecasting_task(config):
    # Long-running forecasting
    return results

# In callback
@app.callback(...)
def start_forecast(n_clicks, config):
    task = run_forecasting_task.delay(config)
    return task.id  # Store task ID, poll for results
```

**Performance Gain**: Non-blocking operations, scalable processing

---

### 3. üìä Chart Optimization

**Problem**: Large datasets cause slow rendering, memory issues
**Impact**: Charts take 5-10 seconds to render, browser lag

**Solution**: Data downsampling, WebGL rendering, lazy loading

```python
import plotly.graph_objects as go
from scipy.signal import resample

def create_optimized_line_chart(x, y, max_points=5000):
    # Downsample if too many points
    if len(x) > max_points:
        indices = np.linspace(0, len(x)-1, max_points, dtype=int)
        x = x[indices]
        y = y[indices]

    # Use scattergl for large datasets (WebGL)
    fig = go.Figure(go.Scattergl(  # Note: Scattergl instead of Scatter
        x=x, y=y,
        mode='lines',
        line=dict(width=1)
    ))

    # Optimize layout
    fig.update_layout(
        uirevision='constant',  # Preserve zoom/pan
        hovermode='closest',
        modebar={'remove': ['lasso', 'select']}  # Remove unused tools
    )

    return fig
```

**Performance Gain**: 50-90% faster chart rendering

---

### 4. üóÑÔ∏è Database Layer for Metadata

**Problem**: File-based storage slow for queries, no indexing
**Impact**: Slow project loading, scenario listing

**Solution**: SQLite for metadata, files for large data

```python
# dash/utils/database.py
import sqlite3
from contextlib import contextmanager

class MetadataDB:
    def __init__(self, db_path='dash/data/metadata.db'):
        self.db_path = db_path
        self._init_db()

    def _init_db(self):
        with self._get_conn() as conn:
            conn.execute('''
                CREATE TABLE IF NOT EXISTS projects (
                    id INTEGER PRIMARY KEY,
                    name TEXT UNIQUE,
                    path TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    last_accessed TIMESTAMP,
                    metadata JSON
                )
            ''')
            conn.execute('''
                CREATE TABLE IF NOT EXISTS scenarios (
                    id INTEGER PRIMARY KEY,
                    project_id INTEGER,
                    name TEXT,
                    type TEXT,  -- forecast, profile, pypsa
                    status TEXT,  -- completed, running, failed
                    created_at TIMESTAMP,
                    metadata JSON,
                    FOREIGN KEY(project_id) REFERENCES projects(id)
                )
            ''')
            conn.execute('CREATE INDEX IF NOT EXISTS idx_project_name ON projects(name)')
            conn.execute('CREATE INDEX IF NOT EXISTS idx_scenario_project ON scenarios(project_id)')

    @contextmanager
    def _get_conn(self):
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        try:
            yield conn
            conn.commit()
        finally:
            conn.close()

    def add_project(self, name, path, metadata=None):
        with self._get_conn() as conn:
            conn.execute(
                'INSERT OR REPLACE INTO projects (name, path, metadata) VALUES (?, ?, ?)',
                (name, path, json.dumps(metadata or {}))
            )

    def get_recent_projects(self, limit=10):
        with self._get_conn() as conn:
            cursor = conn.execute(
                'SELECT * FROM projects ORDER BY last_accessed DESC LIMIT ?',
                (limit,)
            )
            return [dict(row) for row in cursor.fetchall()]
```

**Performance Gain**: 5-10x faster queries, better scalability

---

### 5. üìÑ Pagination & Lazy Loading

**Problem**: Loading all data at once overwhelms browser
**Impact**: Memory issues, slow initial load

**Solution**: Server-side pagination, virtual scrolling

```python
import dash_ag_grid as dag  # pip install dash-ag-grid

def create_paginated_table(df, page_size=50):
    return dag.AgGrid(
        rowData=df.to_dict('records'),
        columnDefs=[{'field': col} for col in df.columns],
        defaultColDef={
            'sortable': True,
            'filter': True,
            'resizable': True
        },
        dashGridOptions={
            'pagination': True,
            'paginationPageSize': page_size,
            'paginationPageSizeSelector': [20, 50, 100, 200],
            'cacheBlockSize': 50,  # Virtual scrolling
            'rowModelType': 'infinite'  # Infinite scroll
        }
    )

# Callback for lazy loading
@app.callback(
    Output('table-data', 'rowData'),
    Input('table', 'virtualRowData'),  # Only load visible rows
    State('active-project', 'data')
)
def load_table_chunk(virtual_rows, project):
    start = virtual_rows['startRow']
    end = virtual_rows['endRow']
    # Load only requested chunk from disk/database
    chunk = load_data_chunk(project, start, end)
    return chunk.to_dict('records')
```

**Performance Gain**: 80% faster initial load, unlimited data size

---

### 6. üéØ Callback Optimization

**Problem**: Too many callbacks firing, unnecessary re-renders
**Impact**: CPU waste, UI lag

**Solution**: Smart dependencies, partial updates, memoization

```python
from dash import callback_context, no_update
from functools import lru_cache

# Use no_update to prevent unnecessary updates
@app.callback(
    Output('chart', 'figure'),
    Output('stats', 'children'),
    Input('scenario-dropdown', 'value'),
    State('active-project', 'data'),
    prevent_initial_call=True
)
def update_visualization(scenario, project):
    if not scenario:
        return no_update, no_update  # Don't update if no scenario

    # Use callback_context to determine what triggered
    ctx = callback_context
    if not ctx.triggered:
        return no_update, no_update

    trigger = ctx.triggered[0]['prop_id'].split('.')[0]

    # Load data (cached)
    data = load_cached_data(project, scenario)

    # Only update what changed
    if trigger == 'scenario-dropdown':
        return create_chart(data), create_stats(data)

    return no_update, no_update

# LRU cache for expensive computations
@lru_cache(maxsize=100)
def compute_statistics(scenario_hash):
    # Expensive statistical computation
    return stats

# Use pattern-matching for dynamic components efficiently
@app.callback(
    Output({'type': 'sector-chart', 'index': MATCH}, 'figure'),
    Input({'type': 'sector-dropdown', 'index': MATCH}, 'value'),
    State({'type': 'sector-chart', 'index': MATCH}, 'id'),
    prevent_initial_call=True
)
def update_sector_chart(value, chart_id):
    # Only updates the specific chart that changed
    return create_chart(value, chart_id['index'])
```

**Performance Gain**: 40-60% reduction in callback executions

---

### 7. üöÄ Production Deployment

**Problem**: Development server slow, single-threaded
**Impact**: Can't handle multiple users, crashes under load

**Solution**: Production WSGI server, multi-worker, compression

```bash
# Install production dependencies
pip install gunicorn gevent
pip install brotli  # Better compression than gzip
```

```python
# dash/wsgi.py
from app import app

application = app.server

# Optimize for production
application.config.update(
    COMPRESS_MIMETYPES=['text/html', 'text/css', 'application/json', 'application/javascript'],
    COMPRESS_LEVEL=6,
    COMPRESS_MIN_SIZE=500,
)
```

```bash
# Run with gunicorn (Linux/Mac)
gunicorn wsgi:application \
    --workers 4 \
    --worker-class gevent \
    --worker-connections 1000 \
    --timeout 300 \
    --bind 0.0.0.0:8050 \
    --access-logfile - \
    --error-logfile - \
    --log-level info

# Windows: use waitress
pip install waitress
waitress-serve --host=0.0.0.0 --port=8050 --threads=8 wsgi:application
```

**Performance Gain**: 5-10x more concurrent users, 2-3x faster responses

---

### 8. üíæ State Management Optimization

**Problem**: Storing large data in dcc.Store causes memory issues
**Impact**: Browser crashes, slow serialization

**Solution**: Store references, not data

```python
# Bad: Store entire DataFrame
dcc.Store(id='data-store', data=df.to_dict('records'))  # ‚ùå 100MB+

# Good: Store file path/hash, load on demand
dcc.Store(id='data-ref-store', data={'path': '/path/to/file.csv', 'hash': 'abc123'})  # ‚úÖ <1KB

# Callback loads data when needed
@app.callback(
    Output('chart', 'figure'),
    Input('data-ref-store', 'data')
)
def update_chart(data_ref):
    if not data_ref:
        return {}

    # Load from cache or disk
    df = load_cached_data(data_ref['hash'], data_ref['path'])
    return create_chart(df)
```

**Performance Gain**: 90% reduction in memory usage

---

### 9. üìä PyPSA Network Optimization

**Problem**: Loading large NetCDF files slow, blocking
**Impact**: 10-30 second delays for network operations

**Solution**: Enhanced caching, lazy loading, compression

```python
# dash/models/network_cache_optimized.py
import pypsa
import hashlib
import pickle
import lz4.frame  # pip install lz4
from pathlib import Path

class OptimizedNetworkCache:
    def __init__(self, cache_dir='dash/data/network_cache', max_size_mb=1000):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.max_size = max_size_mb * 1024 * 1024
        self._memory_cache = {}  # In-memory LRU

    def get_cache_key(self, filepath):
        stat = Path(filepath).stat()
        key_str = f"{filepath}:{stat.st_mtime}:{stat.st_size}"
        return hashlib.md5(key_str.encode()).hexdigest()

    def load_network(self, filepath):
        cache_key = self.get_cache_key(filepath)

        # Check memory cache first
        if cache_key in self._memory_cache:
            return self._memory_cache[cache_key]

        # Check disk cache
        cache_file = self.cache_dir / f"{cache_key}.pkl.lz4"
        if cache_file.exists():
            with open(cache_file, 'rb') as f:
                compressed = f.read()
                data = lz4.frame.decompress(compressed)
                network = pickle.loads(data)
                self._memory_cache[cache_key] = network
                return network

        # Load from source (slow)
        network = pypsa.Network(filepath)

        # Save to cache (compressed)
        data = pickle.dumps(network, protocol=pickle.HIGHEST_PROTOCOL)
        compressed = lz4.frame.compress(data, compression_level=4)
        with open(cache_file, 'wb') as f:
            f.write(compressed)

        self._memory_cache[cache_key] = network
        self._cleanup_cache()

        return network

    def _cleanup_cache(self):
        # Remove old files if cache too large
        total_size = sum(f.stat().st_size for f in self.cache_dir.glob('*'))
        if total_size > self.max_size:
            # Remove oldest files
            files = sorted(self.cache_dir.glob('*'), key=lambda f: f.stat().st_mtime)
            for f in files[:len(files)//2]:
                f.unlink()
```

**Performance Gain**: 100x faster network loading (0.1s vs 10s)

---

### 10. üîç Performance Monitoring

**Problem**: No visibility into slow operations
**Impact**: Can't identify bottlenecks

**Solution**: Built-in profiling and monitoring

```python
# dash/utils/profiler.py
import time
import functools
import logging
from collections import defaultdict

logger = logging.getLogger(__name__)

class PerformanceMonitor:
    def __init__(self):
        self.timings = defaultdict(list)

    def timer(self, name):
        def decorator(func):
            @functools.wraps(func)
            def wrapper(*args, **kwargs):
                start = time.perf_counter()
                result = func(*args, **kwargs)
                elapsed = time.perf_counter() - start

                self.timings[name].append(elapsed)

                # Log slow operations
                if elapsed > 1.0:
                    logger.warning(f"{name} took {elapsed:.2f}s (SLOW)")

                return result
            return wrapper
        return decorator

    def get_stats(self):
        stats = {}
        for name, timings in self.timings.items():
            stats[name] = {
                'count': len(timings),
                'total': sum(timings),
                'avg': sum(timings) / len(timings),
                'min': min(timings),
                'max': max(timings)
            }
        return stats

monitor = PerformanceMonitor()

# Usage
@monitor.timer('load_forecast_data')
def load_forecast_data(path):
    # Your code
    return data
```

---

## üìä Expected Performance Improvements

| Optimization | Before | After | Improvement |
|-------------|--------|-------|-------------|
| Network Loading | 10s | 0.1s | **100x faster** |
| Chart Rendering | 5s | 0.5s | **10x faster** |
| Data Queries | 2s | 0.2s | **10x faster** |
| Concurrent Users | 1-2 | 20-50 | **25x more** |
| Memory Usage | 2GB | 200MB | **90% less** |
| Initial Load | 5s | 1s | **5x faster** |

---

## üõ†Ô∏è Implementation Priority

### High Priority (Immediate Impact)
1. ‚úÖ **Caching Layer** - Easy to add, huge impact
2. ‚úÖ **Chart Optimization** - Use Scattergl, downsample
3. ‚úÖ **Callback Optimization** - Use no_update, smart dependencies
4. ‚úÖ **Production Server** - Switch to Gunicorn

### Medium Priority (Good ROI)
5. **Database Layer** - For metadata and queries
6. **Pagination** - For large tables
7. **State Optimization** - Store references not data

### Low Priority (Complex but Powerful)
8. **Background Tasks** - Celery setup requires Redis
9. **Advanced Monitoring** - Nice to have

---

## üöÄ Quick Start: Apply Top 3 Optimizations

1. **Add Caching** (5 minutes):
```bash
pip install flask-caching
```

2. **Update requirements.txt** (1 minute):
Add `flask-caching>=2.0.0`

3. **Modify app.py** (10 minutes):
See `app_optimized.py` example

**Total time**: ~15 minutes for 5-10x performance boost!
